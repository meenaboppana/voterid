{
 "metadata": {
  "name": "",
  "signature": "sha256:38a0e4e5298b065bf3aabbc097f90f7de715177bebf93ecc26c74933742158c0"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import requests, zipfile, StringIO\n",
      "import pandas as pd\n",
      "from pandas import DataFrame, read_csv\n",
      "import math\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#urls for Voting registration by Sex, Race and Hispanic Origin, for States \n",
      "url_2012 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2012/Table04b.csv'\n",
      "url_2010 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2010/Table4b_2010.csv'\n",
      "url_2008 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2008/Table%2004b.csv'\n",
      "url_2006 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2006/tab04b.csv'\n",
      "url_2004 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2004/tab04a.csv'\n",
      "url_2002 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2002/tab04a.csv'\n",
      "urllist=[url_2012, url_2010, url_2008, url_2006, url_2004, url_2002]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#IGNORE ME\n",
      "url = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2004/tab04a.csv'\n",
      "table=pd.read_csv(StringIO.StringIO(requests.get(url).content),error_bad_lines=False)\n",
      "\n",
      "table.ix[4,table.columns[0:3]]=table.ix[3,table.columns[0:3]]\n",
      "colnames=dict(zip(table.columns, table.ix[4,:]))\n",
      "table.rename(columns=colnames, inplace=True)\n",
      "table=table.drop([0,1,2,3,4]).reset_index(drop=True)\n",
      "table.ix[0,0]='All'\n",
      "table=table.ix[:,(~pd.isnull(table.columns))]\n",
      "table=table.ix[:,[0,1,2,5,6,7,8,9,10]]\n",
      "table.columns=df.columns[[1,2,3,4,5,6,9,10,11]]\n",
      "table['Year'] = year\n",
      "table['Percent registered\\\\(Citizen)']='nan'\n",
      "table['Margin of Error2']='nan'\n",
      "table['Percent voted\\\\(Citizen)']='nan'\n",
      "table['Margin of Error4']='nan'\n",
      "table['State']='nan'\n",
      "table=table.drop([43]).reset_index(drop=True)#one state has a duplicate row\n",
      "for i in range(49):      \n",
      "    table[13*i:13*(i+1)]['State'] = table.ix[13*i]['Race and Hispanic origin']\n",
      "table = table[df.columns]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Skipping line 657: expected 13 fields, saw 24\n",
        "Skipping line 658: expected 13 fields, saw 24\n",
        "Skipping line 659: expected 13 fields, saw 24\n",
        "Skipping line 660: expected 13 fields, saw 24\n",
        "Skipping line 661: expected 13 fields, saw 24\n",
        "Skipping line 662: expected 13 fields, saw 24\n",
        "Skipping line 663: expected 13 fields, saw 24\n",
        "Skipping line 664: expected 13 fields, saw 24\n",
        "Skipping line 665: expected 13 fields, saw 24\n",
        "Skipping line 666: expected 13 fields, saw 24\n",
        "Skipping line 667: expected 13 fields, saw 24\n",
        "Skipping line 668: expected 13 fields, saw 24\n",
        "Skipping line 669: expected 13 fields, saw 24\n",
        "Skipping line 670: expected 13 fields, saw 24\n",
        "Skipping line 671: expected 13 fields, saw 24\n",
        "Skipping line 672: expected 13 fields, saw 24\n",
        "Skipping line 673: expected 13 fields, saw 37\n",
        "Skipping line 674: expected 13 fields, saw 37\n",
        "Skipping line 675: expected 13 fields, saw 37\n",
        "Skipping line 676: expected 13 fields, saw 37\n",
        "Skipping line 677: expected 13 fields, saw 37\n",
        "Skipping line 678: expected 13 fields, saw 37\n",
        "Skipping line 679: expected 13 fields, saw 37\n",
        "Skipping line 680: expected 13 fields, saw 37\n",
        "Skipping line 681: expected 13 fields, saw 37\n",
        "Skipping line 682: expected 13 fields, saw 37\n",
        "Skipping line 683: expected 13 fields, saw 37\n",
        "Skipping line 684: expected 13 fields, saw 37\n",
        "Skipping line 685: expected 13 fields, saw 37\n",
        "Skipping line 686: expected 13 fields, saw 37\n",
        "Skipping line 687: expected 13 fields, saw 37\n",
        "Skipping line 688: expected 13 fields, saw 37\n",
        "\n"
       ]
      },
      {
       "ename": "NameError",
       "evalue": "name 'df' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-4-1c6d76ca8ff1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m~\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m11\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Year'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Percent registered\\\\(Citizen)'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'nan'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
       ]
      }
     ],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#DIFFERENT VERSION OF NEXT BOX: CREATES ONE LARGE DATAFRAME INSTEAD\n",
      "#TOTAL VOTER INCLUDES NON CITIZENS (BOTH 18+)\n",
      "#WE'RE WORKING WITH CITIZENS\n",
      "\n",
      "#iterate through years\n",
      "for year in [2012, 2010, 2008, 2006]:\n",
      "    \n",
      "    #create dataframe\n",
      "    url=urllist[(2012-year)/2]\n",
      "    table=pd.read_csv(StringIO.StringIO(requests.get(url).content),error_bad_lines=False)\n",
      "    \n",
      "    #create large dataframe\n",
      "    #each year needs own case b/c original tables formatted differently; combined to reuse code wherever possible \n",
      "    \n",
      "    if year==2012:\n",
      "        changeCol=dict(zip(table.columns, table.ix[2,:]))#change the columns to match what we want them to be \n",
      "        table.rename(columns=changeCol, inplace=True)\n",
      "        table=table.drop([0,1,2]).reset_index(drop=True)\n",
      "        table=table[table.index < 572] # we don't want first or last few rows\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))] #removing extra nan columns\n",
      "        table['Year'] = year\n",
      "        for i in range(52):\n",
      "            table[11*i:11*(i+1)]['State'] = table.ix[11*i]['State']\n",
      "        df=table\n",
      "        df.columns=['State', u'Race and Hispanic origin', u'Total Population', u'Total Citizen Population', u'Total registered', u'Percent registered\\n(Total)', u'Margin of Error1', u'Percent registered\\n(Citizen)', u'Margin of Error2', u'Total voted', u'Percent voted\\n(Total)', u'Margin of Error3', u'Percent voted\\n(Citizen)', u'Margin of Error4', u'Year']\n",
      "    \n",
      "    elif year==2010:\n",
      "        table.ix[1, 1]='Race and Hispanic origin'\n",
      "        colnames=dict(zip(table.columns, table.ix[1,:]))#not analagous for years\n",
      "        table.rename(columns=colnames, inplace=True)\n",
      "        table=table.drop([0,1]).reset_index(drop=True) #also not analagous\n",
      "        table=table[table.index < 572]\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))]\n",
      "        table['Year'] = year\n",
      "        table.ix[0, 'STATE']='All'\n",
      "        state = []\n",
      "        for i in range(52):\n",
      "            table[11*i:11*(i+1)]['STATE'] = table.ix[11*i]['STATE']\n",
      "            state.append(table.ix[11*i]['STATE'])\n",
      "        table.columns = df.columns\n",
      "        df = pd.concat([table,df])\n",
      "    \n",
      "    elif (year==2008)|(year==2006):\n",
      "        table.ix[4,table.columns[0:3]]=table.ix[3,table.columns[0:3]]\n",
      "        colnames=dict(zip(table.columns, table.ix[4,:]))\n",
      "        table.rename(columns=colnames, inplace=True)\n",
      "        table=table.drop([0,1,2,3,4]).reset_index(drop=True)\n",
      "        table.ix[0,0]='All'\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))]\n",
      "        table['Year'] = year\n",
      "        table.columns=df.columns[1:15]\n",
      "        table['State']='nan'\n",
      "        for i in range(52):      \n",
      "            table[12*i:12*(i+1)]['State'] = table.ix[12*i]['Race and Hispanic origin']   \n",
      "        #delete every 12th row starting from first\n",
      "        cols = table.columns.tolist()\n",
      "        cols = cols[-1x:] + cols[:-1]\n",
      "        table = table[cols]\n",
      "        df = pd.concat([table,df])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#THIS SHOULD BE MADE MORE ACCURATE IF SOMEONE IS BORED\n",
      "\n",
      "#http://www.ncsl.org/research/elections-and-campaigns/voter-id-history.aspx\n",
      "    #0 = no voter id law\n",
      "    #1 = not strict voter id law (w/ or w/out photo)\n",
      "    #2 = strict voter id law (w/ or w/out photo)\n",
      "#http://en.wikipedia.org/wiki/Voter_ID_laws_in_the_United_States\n",
      "#http://www.hartwick.edu/Documents/POSC/MeyerThesisSp13.pdf -- not completely synced with this source\n",
      "\n",
      "law_id=pd.DataFrame(columns=['State','non_strict_year', 'strict_year'])\n",
      "law_id['State'] = state[1:52]\n",
      "\n",
      "law_id.ix[0,1] = 2003\n",
      "law_id.ix[1,1] = 1980\n",
      "law_id.ix[2,2] = 2004 #check wiki article date: 2006?\n",
      "law_id.ix[5,1] = 2003\n",
      "law_id.ix[6,1] = 2010\n",
      "law_id.ix[7,1] = 2010\n",
      "law_id.ix[9,1] = 1977\n",
      "law_id.ix[10,[1,2]] = [1997,2008]\n",
      "law_id.ix[11,1] = 1970\n",
      "law_id.ix[12,1] = 2010\n",
      "law_id.ix[14,2] = 2008\n",
      "law_id.ix[16,2] = 2011\n",
      "law_id.ix[18,1] = 2010\n",
      "law_id.ix[22,1] = 2010\n",
      "law_id.ix[24,2] = 2012\n",
      "law_id.ix[25,1] = 2002\n",
      "law_id.ix[26,1] = 2003\n",
      "law_id.ix[29,1] = 2012\n",
      "law_id.ix[33,2] = 2016\n",
      "law_id.ix[34,[1,2]] = [2003,2013]\n",
      "law_id.ix[35,2] = 2006\n",
      "law_id.ix[36,1] = 2010\n",
      "law_id.ix[39,1] = 2013\n",
      "law_id.ix[40,[1,2]] = [1950,2013]\n",
      "law_id.ix[41,1] = 2003\n",
      "law_id.ix[42,[1,2]] = [1990,2011]\n",
      "law_id.ix[43,1] = 1971\n",
      "law_id.ix[44,1] = 2009\n",
      "law_id.ix[46,[1,2]] = [1996,2013]\n",
      "law_id.ix[47,1] = 2005\n",
      "law_id"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>State</th>\n",
        "      <th>non_strict_year</th>\n",
        "      <th>strict_year</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0 </th>\n",
        "      <td>              ALABAMA</td>\n",
        "      <td> 2003</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1 </th>\n",
        "      <td>               ALASKA</td>\n",
        "      <td> 1980</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2 </th>\n",
        "      <td>              ARIZONA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2004</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3 </th>\n",
        "      <td>             ARKANSAS</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4 </th>\n",
        "      <td>           CALIFORNIA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>5 </th>\n",
        "      <td>             COLORADO</td>\n",
        "      <td> 2003</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>6 </th>\n",
        "      <td>          CONNECTICUT</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>7 </th>\n",
        "      <td>             DELAWARE</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>8 </th>\n",
        "      <td> DISTRICT OF COLUMBIA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>9 </th>\n",
        "      <td>              FLORIDA</td>\n",
        "      <td> 1977</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10</th>\n",
        "      <td>              GEORGIA</td>\n",
        "      <td> 1997</td>\n",
        "      <td> 2008</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>11</th>\n",
        "      <td>               HAWAII</td>\n",
        "      <td> 1970</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>12</th>\n",
        "      <td>                IDAHO</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>13</th>\n",
        "      <td>             ILLINOIS</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>14</th>\n",
        "      <td>              INDIANA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2008</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>15</th>\n",
        "      <td>                 IOWA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>16</th>\n",
        "      <td>               KANSAS</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2011</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>17</th>\n",
        "      <td>             KENTUCKY</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>18</th>\n",
        "      <td>            LOUISIANA</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>19</th>\n",
        "      <td>                MAINE</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>20</th>\n",
        "      <td>             MARYLAND</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>21</th>\n",
        "      <td>        MASSACHUSETTS</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>22</th>\n",
        "      <td>             MICHIGAN</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>23</th>\n",
        "      <td>            MINNESOTA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>24</th>\n",
        "      <td>          MISSISSIPPI</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2012</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>25</th>\n",
        "      <td>             MISSOURI</td>\n",
        "      <td> 2002</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>26</th>\n",
        "      <td>              MONTANA</td>\n",
        "      <td> 2003</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>27</th>\n",
        "      <td>             NEBRASKA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>28</th>\n",
        "      <td>               NEVADA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>29</th>\n",
        "      <td>        NEW HAMPSHIRE</td>\n",
        "      <td> 2012</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>30</th>\n",
        "      <td>           NEW JERSEY</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>31</th>\n",
        "      <td>           NEW MEXICO</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>32</th>\n",
        "      <td>             NEW YORK</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>33</th>\n",
        "      <td>       NORTH CAROLINA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2016</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>34</th>\n",
        "      <td>         NORTH DAKOTA</td>\n",
        "      <td> 2003</td>\n",
        "      <td> 2013</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>35</th>\n",
        "      <td>                 OHIO</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2006</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>36</th>\n",
        "      <td>             OKLAHOMA</td>\n",
        "      <td> 2010</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>37</th>\n",
        "      <td>               OREGON</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>38</th>\n",
        "      <td>         PENNSYLVANIA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>39</th>\n",
        "      <td>         RHODE ISLAND</td>\n",
        "      <td> 2013</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>40</th>\n",
        "      <td>       SOUTH CAROLINA</td>\n",
        "      <td> 1950</td>\n",
        "      <td> 2013</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>41</th>\n",
        "      <td>         SOUTH DAKOTA</td>\n",
        "      <td> 2003</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>42</th>\n",
        "      <td>            TENNESSEE</td>\n",
        "      <td> 1990</td>\n",
        "      <td> 2011</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>43</th>\n",
        "      <td>                TEXAS</td>\n",
        "      <td> 1971</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>44</th>\n",
        "      <td>                 UTAH</td>\n",
        "      <td> 2009</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>45</th>\n",
        "      <td>              VERMONT</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>46</th>\n",
        "      <td>             VIRGINIA</td>\n",
        "      <td> 1996</td>\n",
        "      <td> 2013</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>47</th>\n",
        "      <td>           WASHINGTON</td>\n",
        "      <td> 2005</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>48</th>\n",
        "      <td>        WEST VIRGINIA</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>49</th>\n",
        "      <td>            WISCONSIN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>50</th>\n",
        "      <td>              WYOMING</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 75,
       "text": [
        "                   State non_strict_year strict_year\n",
        "0                ALABAMA            2003         NaN\n",
        "1                 ALASKA            1980         NaN\n",
        "2                ARIZONA             NaN        2004\n",
        "3               ARKANSAS             NaN         NaN\n",
        "4             CALIFORNIA             NaN         NaN\n",
        "5               COLORADO            2003         NaN\n",
        "6            CONNECTICUT            2010         NaN\n",
        "7               DELAWARE            2010         NaN\n",
        "8   DISTRICT OF COLUMBIA             NaN         NaN\n",
        "9                FLORIDA            1977         NaN\n",
        "10               GEORGIA            1997        2008\n",
        "11                HAWAII            1970         NaN\n",
        "12                 IDAHO            2010         NaN\n",
        "13              ILLINOIS             NaN         NaN\n",
        "14               INDIANA             NaN        2008\n",
        "15                  IOWA             NaN         NaN\n",
        "16                KANSAS             NaN        2011\n",
        "17              KENTUCKY             NaN         NaN\n",
        "18             LOUISIANA            2010         NaN\n",
        "19                 MAINE             NaN         NaN\n",
        "20              MARYLAND             NaN         NaN\n",
        "21         MASSACHUSETTS             NaN         NaN\n",
        "22              MICHIGAN            2010         NaN\n",
        "23             MINNESOTA             NaN         NaN\n",
        "24           MISSISSIPPI             NaN        2012\n",
        "25              MISSOURI            2002         NaN\n",
        "26               MONTANA            2003         NaN\n",
        "27              NEBRASKA             NaN         NaN\n",
        "28                NEVADA             NaN         NaN\n",
        "29         NEW HAMPSHIRE            2012         NaN\n",
        "30            NEW JERSEY             NaN         NaN\n",
        "31            NEW MEXICO             NaN         NaN\n",
        "32              NEW YORK             NaN         NaN\n",
        "33        NORTH CAROLINA             NaN        2016\n",
        "34          NORTH DAKOTA            2003        2013\n",
        "35                  OHIO             NaN        2006\n",
        "36              OKLAHOMA            2010         NaN\n",
        "37                OREGON             NaN         NaN\n",
        "38          PENNSYLVANIA             NaN         NaN\n",
        "39          RHODE ISLAND            2013         NaN\n",
        "40        SOUTH CAROLINA            1950        2013\n",
        "41          SOUTH DAKOTA            2003         NaN\n",
        "42             TENNESSEE            1990        2011\n",
        "43                 TEXAS            1971         NaN\n",
        "44                  UTAH            2009         NaN\n",
        "45               VERMONT             NaN         NaN\n",
        "46              VIRGINIA            1996        2013\n",
        "47            WASHINGTON            2005         NaN\n",
        "48         WEST VIRGINIA             NaN         NaN\n",
        "49             WISCONSIN             NaN         NaN\n",
        "50               WYOMING             NaN         NaN"
       ]
      }
     ],
     "prompt_number": 75
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "racedata = pd.merge(df, law_id, on = ['State'])\n",
      "\n",
      "#boolean shtuff\n",
      "racedata['status'] = 2 * (racedata['strict_year']<racedata['Year']).astype(int) + (racedata['non_strict_year']<racedata['Year']).astype(int) \n",
      "\n",
      "#Change 3's to 2's to make up for the fact that we added states that had both strict and non strict id laws twice\n",
      "for row in racedata.index:\n",
      "    if racedata.ix[row,'status'] == 3:\n",
      "        racedata.ix[row,'status'] = 2\n",
      "        \n",
      "racedata.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>State</th>\n",
        "      <th>Race and Hispanic origin</th>\n",
        "      <th>Total Population</th>\n",
        "      <th>Total Citizen Population</th>\n",
        "      <th>Total registered</th>\n",
        "      <th>Percent registered\n",
        "(Total)</th>\n",
        "      <th>Margin of Error1</th>\n",
        "      <th>Percent registered\n",
        "(Citizen)</th>\n",
        "      <th>Margin of Error2</th>\n",
        "      <th>Total voted</th>\n",
        "      <th>Percent voted\n",
        "(Total)</th>\n",
        "      <th>Margin of Error3</th>\n",
        "      <th>Percent voted\n",
        "(Citizen)</th>\n",
        "      <th>Margin of Error4</th>\n",
        "      <th>Year</th>\n",
        "      <th>non_strict_year</th>\n",
        "      <th>strict_year</th>\n",
        "      <th>status</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td> ALABAMA</td>\n",
        "      <td>      ALABAMA</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>   NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 2006</td>\n",
        "      <td> 2003</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td> ALABAMA</td>\n",
        "      <td>       .Total</td>\n",
        "      <td> 3,447</td>\n",
        "      <td> 3,353</td>\n",
        "      <td> 2,480</td>\n",
        "      <td> 72.0</td>\n",
        "      <td> 2.1</td>\n",
        "      <td> 74.0</td>\n",
        "      <td> 2.0</td>\n",
        "      <td> 1,667</td>\n",
        "      <td> 48.4</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 49.7</td>\n",
        "      <td> 2.3</td>\n",
        "      <td> 2006</td>\n",
        "      <td> 2003</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> ALABAMA</td>\n",
        "      <td>        .Male</td>\n",
        "      <td> 1,638</td>\n",
        "      <td> 1,566</td>\n",
        "      <td> 1,187</td>\n",
        "      <td> 72.5</td>\n",
        "      <td> 3.0</td>\n",
        "      <td> 75.8</td>\n",
        "      <td> 2.9</td>\n",
        "      <td>   792</td>\n",
        "      <td> 48.4</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 50.6</td>\n",
        "      <td> 3.4</td>\n",
        "      <td> 2006</td>\n",
        "      <td> 2003</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td> ALABAMA</td>\n",
        "      <td>      .Female</td>\n",
        "      <td> 1,809</td>\n",
        "      <td> 1,786</td>\n",
        "      <td> 1,293</td>\n",
        "      <td> 71.5</td>\n",
        "      <td> 2.9</td>\n",
        "      <td> 72.4</td>\n",
        "      <td> 2.9</td>\n",
        "      <td>   875</td>\n",
        "      <td> 48.4</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 49.0</td>\n",
        "      <td> 3.2</td>\n",
        "      <td> 2006</td>\n",
        "      <td> 2003</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td> ALABAMA</td>\n",
        "      <td> .White alone</td>\n",
        "      <td> 2,500</td>\n",
        "      <td> 2,430</td>\n",
        "      <td> 1,824</td>\n",
        "      <td> 73.0</td>\n",
        "      <td> 2.4</td>\n",
        "      <td> 75.1</td>\n",
        "      <td> 2.4</td>\n",
        "      <td> 1,241</td>\n",
        "      <td> 49.6</td>\n",
        "      <td> 2.7</td>\n",
        "      <td> 51.1</td>\n",
        "      <td> 2.7</td>\n",
        "      <td> 2006</td>\n",
        "      <td> 2003</td>\n",
        "      <td> NaN</td>\n",
        "      <td> 1</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 131,
       "text": [
        "     State Race and Hispanic origin Total Population Total Citizen Population  \\\n",
        "0  ALABAMA                  ALABAMA              NaN                      NaN   \n",
        "1  ALABAMA                   .Total            3,447                    3,353   \n",
        "2  ALABAMA                    .Male            1,638                    1,566   \n",
        "3  ALABAMA                  .Female            1,809                    1,786   \n",
        "4  ALABAMA             .White alone            2,500                    2,430   \n",
        "\n",
        "  Total registered Percent registered\\n(Total) Margin of Error1  \\\n",
        "0              NaN                         NaN              NaN   \n",
        "1            2,480                        72.0              2.1   \n",
        "2            1,187                        72.5              3.0   \n",
        "3            1,293                        71.5              2.9   \n",
        "4            1,824                        73.0              2.4   \n",
        "\n",
        "  Percent registered\\n(Citizen) Margin of Error2 Total voted  \\\n",
        "0                           NaN              NaN         NaN   \n",
        "1                          74.0              2.0       1,667   \n",
        "2                          75.8              2.9         792   \n",
        "3                          72.4              2.9         875   \n",
        "4                          75.1              2.4       1,241   \n",
        "\n",
        "  Percent voted\\n(Total) Margin of Error3 Percent voted\\n(Citizen)  \\\n",
        "0                    NaN              NaN                      NaN   \n",
        "1                   48.4              2.3                     49.7   \n",
        "2                   48.4              3.4                     50.6   \n",
        "3                   48.4              3.2                     49.0   \n",
        "4                   49.6              2.7                     51.1   \n",
        "\n",
        "  Margin of Error4  Year non_strict_year strict_year  status  \n",
        "0              NaN  2006            2003         NaN       1  \n",
        "1              2.3  2006            2003         NaN       1  \n",
        "2              3.4  2006            2003         NaN       1  \n",
        "3              3.2  2006            2003         NaN       1  \n",
        "4              2.7  2006            2003         NaN       1  "
       ]
      }
     ],
     "prompt_number": 131
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#general comment (Kate): there are functions below(ie SplitStatebyDot) which could have made this portion easier\n",
      "#but I didn't make them until I was done with this portion. \n",
      "\n",
      "#iterate through years\n",
      "for year in [2012, 2010, 2008, 2006, 2004, 2002]:\n",
      "    #create dataframe\n",
      "    url=urllist[(2012-year)/2]\n",
      "    table=pd.read_csv(StringIO.StringIO(requests.get(url).content),error_bad_lines=False)\n",
      "    \n",
      "    #create dictionary for info on each state\n",
      "    #each year needs own case b/c original tables formatted differently; combined to reuse code wherever possible \n",
      "    \n",
      "    if (year==2012):\n",
      "        colnames=dict(zip(table.columns, table.ix[2,:]))#change column names \n",
      "        table.rename(columns=colnames, inplace=True)\n",
      "        table=table.drop([0,1,2]).reset_index(drop=True) #remove first and last few rows\n",
      "        table.drop(range(572,580)) \n",
      "        table=table.ix[:,(~pd.isnull(table.columns))] #remove extra nan columns\n",
      "        stateDictRace=dict(zip([table.ix[11*i,0] for i in range(52)], [table[11*i:11*(i+1)] for i in range(52)]))\n",
      "            \n",
      "    elif year==2010:\n",
      "        colnames=dict(zip(table.columns, table.ix[1,:]))#not analagous for years\n",
      "        table.rename(columns=colnames, inplace=True)\n",
      "        table=table.drop([0,1]).reset_index(drop=True) #also not analagous\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))]\n",
      "        table.ix[0, 'STATE']='All'\n",
      "        for i in range(52):\n",
      "               stateDictRace[table.ix[11*i,0]]=[stateDictRace[table.ix[11*i,0]], table[11*i:11*(i+1)]] #modify the dictionary\n",
      "                \n",
      "    elif (year==2008)|(year==2006)|(year==2004)|(year==2002):\n",
      "        table.ix[4,table.columns[0:3]]=table.ix[3,table.columns[0:3]]\n",
      "        changeCol=dict(zip(table.columns, table.ix[4,:]))#change the columns to match what we want them to be #this portion also not analgous for years\n",
      "        table.rename(columns=changeCol, inplace=True)\n",
      "        table=table.drop([0,1,2,3,4]).reset_index(drop=True)#ditto this part\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))] #removing extra nan columns\n",
      "        if year==2002:\n",
      "            for i in range(52):\n",
      "                table.ix[0, 'STATE']='All'\n",
      "                existinglist=stateDictRace[table.ix[10*i,0]]#modify the dictionary again\n",
      "                existinglist.append(table[10*i:10*(i+1)])\n",
      "                stateDictRace[table.ix[10*i,0]]=existinglist\n",
      "        else:\n",
      "            table.ix[0, 'State, sex, race, and Hispanic origin']='All'\n",
      "        if year==2004:\n",
      "            table=table.drop([43]).reset_index(drop=True)#one state has a duplicate row\n",
      "            for i in range(49): #something weird - don't have data for all states in 2004 (missing West Virginia, Wymoning, Wisconsin?)\n",
      "                existinglist=stateDictRace[table.ix[13*i,0]]\n",
      "                existinglist.append(table[13*i:13*(i+1)])\n",
      "                stateDictRace[table.ix[13*i,0]]=existinglist\n",
      "        elif (year==2006)|(year==2008):\n",
      "            for i in range(52):\n",
      "                existinglist=stateDictRace[table.ix[12*i,0]]\n",
      "                existinglist.append(table[12*i:12*(i+1)])\n",
      "                stateDictRace[table.ix[12*i,0]]=existinglist"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Skipping line 657: expected 13 fields, saw 24\n",
        "Skipping line 658: expected 13 fields, saw 24\n",
        "Skipping line 659: expected 13 fields, saw 24\n",
        "Skipping line 660: expected 13 fields, saw 24\n",
        "Skipping line 661: expected 13 fields, saw 24\n",
        "Skipping line 662: expected 13 fields, saw 24\n",
        "Skipping line 663: expected 13 fields, saw 24\n",
        "Skipping line 664: expected 13 fields, saw 24\n",
        "Skipping line 665: expected 13 fields, saw 24\n",
        "Skipping line 666: expected 13 fields, saw 24\n",
        "Skipping line 667: expected 13 fields, saw 24\n",
        "Skipping line 668: expected 13 fields, saw 24\n",
        "Skipping line 669: expected 13 fields, saw 24\n",
        "Skipping line 670: expected 13 fields, saw 24\n",
        "Skipping line 671: expected 13 fields, saw 24\n",
        "Skipping line 672: expected 13 fields, saw 24\n",
        "Skipping line 673: expected 13 fields, saw 37\n",
        "Skipping line 674: expected 13 fields, saw 37\n",
        "Skipping line 675: expected 13 fields, saw 37\n",
        "Skipping line 676: expected 13 fields, saw 37\n",
        "Skipping line 677: expected 13 fields, saw 37\n",
        "Skipping line 678: expected 13 fields, saw 37\n",
        "Skipping line 679: expected 13 fields, saw 37\n",
        "Skipping line 680: expected 13 fields, saw 37\n",
        "Skipping line 681: expected 13 fields, saw 37\n",
        "Skipping line 682: expected 13 fields, saw 37\n",
        "Skipping line 683: expected 13 fields, saw 37\n",
        "Skipping line 684: expected 13 fields, saw 37\n",
        "Skipping line 685: expected 13 fields, saw 37\n",
        "Skipping line 686: expected 13 fields, saw 37\n",
        "Skipping line 687: expected 13 fields, saw 37\n",
        "Skipping line 688: expected 13 fields, saw 37\n",
        "\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this function takes in a table where states are distinguished from demographic information by a dot (.) in front of the latter, returning a list of states and a list of tables for each state\n",
      "def splitbyStateDot(table):\n",
      "    indices=pd.DataFrame([item[0]=='.' for item in table[table.columns[0]][0:(len(table)-5)]])#trying to find states automatically\n",
      "    indices=indices.append([False, False, False, False, False]).reset_index(drop=True)\n",
      "    firstcol=table[table.columns[0]].reset_index(drop=True)\n",
      "    statesLoc=firstcol[indices[0]==False].index\n",
      "    return [item for item in firstcol[indices[0]==False]], [table.ix[statesLoc[i]:(statesLoc[i+1]-1), :] for i in range(len(statesLoc)-1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#this function takes in a table where states are distinguished from demographic information by nan in front of the latter, returning a list of states and a list of tables for each state\n",
      "\n",
      "def splitbyStateNull(table):\n",
      "    indices=table[table.columns[0]].isnull()\n",
      "    indices[len(indices)-5:len(indices)]=False\n",
      "    firstcol=table[table.columns[0]]\n",
      "    statesLoc=firstcol[indices==False].index\n",
      "    return [item for item in firstcol[indices==False]], [table.ix[statesLoc[i]:(statesLoc[i+1]-1), :] for i in range(len(statesLoc)-1)]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#urls for Voting registration by AGE, for States, for years 2002, 2004, 2008, 2010, 2012\n",
      "#NOTE: Missing the year 2006 due to a gap in the data\n",
      "url2_2012 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2012/Table04c.csv'\n",
      "url2_2010 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2010/Table4c_2010.csv'\n",
      "url2_2008 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2008/Table%2004c.csv'\n",
      "url2_2006 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2006/tab04a.csv'\n",
      "url2_2004 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2004/tab04b.csv'\n",
      "url2_2002 = 'http://www.census.gov/hhes/www/socdemo/voting/publications/p20/2002/tab04b.csv'\n",
      "urllist2=[url2_2012, url2_2010, url2_2008, url2_2006, url2_2004, url2_2002]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Defines stateDictAge and stateDictRace\n",
      "\n",
      "for year in [2012, 2010, 2008, 2006, 2004]:\n",
      "    url=urllist2[(2012-year)/2]\n",
      "    table=pd.read_csv(StringIO.StringIO(requests.get(url).content),error_bad_lines=False)\n",
      "    if (year==2012):\n",
      "        changeCol=dict(zip(table.columns, table.ix[2,:]))#change the columns to match what we want them to be \n",
      "        table.rename(columns=changeCol, inplace=True)\n",
      "        table=table.drop([0,1,2]).reset_index(drop=True)\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))] #removing extra nan columns\n",
      "        states, stateTable=splitbyStateNull(table)#getting states and tables for each state\n",
      "        stateDictAge=dict(zip(states[0:52],stateTable[0:52]))#making a dictionary for them\n",
      "    elif year==2010:\n",
      "        table.ix[1, table.columns[1]]=table.ix[2, table.columns[1]]#dealing with nan entry\n",
      "        changeCol=dict(zip(table.columns, table.ix[1,:]))#change the columns to match what we want them to be #this portion also not analgous for years\n",
      "        table.rename(columns=changeCol, inplace=True)\n",
      "        table=table.drop([0,1]).reset_index(drop=True)\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))] #removing extra nan columns\n",
      "        states, stateTable=splitbyStateNull(table) \n",
      "        for i in range(len(stateDictAge.keys())):#modifying the dictionary to include this year\n",
      "            existinglist=stateDictAge[states[i]]\n",
      "            existinglist=[existinglist, stateTable[i]]\n",
      "            stateDictAge[states[i]]=existinglist\n",
      "    elif (year==2008)|(year==2006)|(year==2004):\n",
      "        table.ix[4,table.columns[0:3]]=table.ix[3,table.columns[0:3]]\n",
      "        changeCol=dict(zip(table.columns, table.ix[4,:]))#change the columns to match what we want them to be #this portion also not analgous for years\n",
      "        table.rename(columns=changeCol, inplace=True)\n",
      "        table=table.drop([0,1,2,3,4]).reset_index(drop=True)#ditto this part\n",
      "        table=table.ix[:,(~pd.isnull(table.columns))]#removes null columns\n",
      "        table=table[~pd.isnull(table[table.columns[0]])]#removes null rows\n",
      "        table.ix[0, table.columns[0]]='US'#one entry header not the same as previous years\n",
      "        states, stateTable=splitbyStateDot(table)\n",
      "        splitStates=[item.split(' ') for item in states]#removing random 2s in this year's table\n",
      "        states=[' '.join([i for i in item if not i.isdigit()]) for item in splitStates]\n",
      "        for i in range(len(stateDictAge.keys())):#modifying the dictionary again\n",
      "            existinglist=stateDictAge[states[i]]\n",
      "            existinglist.append(stateTable[i])\n",
      "            stateDictRace[states[i]]=existinglist\n",
      "    if year==2006:\n",
      "        print 'I could not find data for 2006, so this is 2004 instead.'\n",
      "    if year==2004:\n",
      "        print 'Similarly, 2004 is actually 2002.'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "I could not find data for 2006, so this is 2004 instead.\n",
        "Similarly, 2004 is actually 2002."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Everything below this point is *only for debugging* - don't worry if it doesn't work. "
     ]
    }
   ],
   "metadata": {}
  }
 ]
}